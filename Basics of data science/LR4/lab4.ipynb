{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/maxd/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/maxd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import emoji\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd = pd.read_csv('data/rusentitweet_train.csv')\n",
    "test_pd = pd.read_csv('data/rusentitweet_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = ['positive', 'negative']\n",
    "train_pd_f = train_pd[train_pd['label'].isin(options)]\n",
    "test_pd_f = test_pd[test_pd['label'].isin(options)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_regex():\n",
    "    emojis = sorted(emoji.EMOJI_DATA, key=len, reverse=True)\n",
    "    emoji_pattern = ''.join(re.escape(u) for u in emojis)\n",
    "    pattern = '[^\\w\\s!?' + emoji_pattern + ']'\n",
    "    return re.compile(pattern)\n",
    "\n",
    "def preprocess(text):\n",
    "    # удаление тегов\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "\n",
    "    # удаление ссылок\n",
    "    text = re.sub(r\"(https|http):\\/\\/\\S+\", \"\", text)\n",
    "\n",
    "    # удаление всего, кроме слов, эмоджи и знаков '!', '?'\n",
    "    filter = filter_regex()\n",
    "    text = filter.sub(repl='', string=text)\n",
    "\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"ё\", \"e\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_734/84643341.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_pd_f['text'] = train_pd_f['text'].apply(preprocess)\n",
      "/tmp/ipykernel_734/84643341.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_pd_f['text'] = test_pd_f['text'].apply(preprocess)\n"
     ]
    }
   ],
   "source": [
    "train_pd_f['text'] = train_pd_f['text'].apply(preprocess)\n",
    "test_pd_f['text'] = test_pd_f['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemm(text):\n",
    "    stemmer = SnowballStemmer(\"russian\")\n",
    "    stop_words = stopwords.words(\"russian\")\n",
    "    emojies = emoji.distinct_emoji_list(text)\n",
    "    text = emoji.replace_emoji(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            stemmed_tokens.append(stemmer.stem(token))\n",
    "    stemmed_tokens += emojies\n",
    "    return ' '.join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_train = train_pd_f.copy()\n",
    "stemmed_train['text'] = stemmed_train['text'].apply(stemm)\n",
    "\n",
    "stemmend_test = test_pd_f.copy()\n",
    "stemmend_test['text'] = stemmend_test['text'].apply(stemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "x_train_stemmed = vectorizer.fit_transform(stemmed_train['text'])\n",
    "x_test_stemmed = vectorizer.transform(stemmend_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "x_train_stemmend_tfidf = tfidf_transformer.fit_transform(x_train_stemmed)\n",
    "x_test_stemmend_tfidf = tfidf_transformer.transform(x_test_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapper = {'positive' : 1, 'negative' : 0}\n",
    "\n",
    "y_train = train_pd_f['label'].map(label_mapper)\n",
    "y_test = test_pd_f['label'].map(label_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_stemmed = LogisticRegression()\n",
    "lr_stemmed.fit(x_train_stemmend_tfidf, y_train)\n",
    "y_predicted_lr_stemmed = lr_stemmed.predict(x_test_stemmend_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_stemmed = RandomForestClassifier()\n",
    "forest_stemmed.fit(x_train_stemmend_tfidf, y_train)\n",
    "y_predicted_forest_stemmed = forest_stemmed.predict(x_test_stemmend_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.93      0.82       660\n",
      "           1       0.84      0.53      0.65       483\n",
      "\n",
      "    accuracy                           0.76      1143\n",
      "   macro avg       0.78      0.73      0.73      1143\n",
      "weighted avg       0.77      0.76      0.74      1143\n",
      "\n",
      "ROC-AUC: 0.726853943158291\n",
      "\n",
      "Пять слов с набольшим позитивным окрасом:\n",
      "[]\n",
      "Пять слов с набольшим негативным окрасом:\n",
      "[('любл', 3.522681141616364), ('красив', 3.462515274065095), ('прекрасн', 2.802872553173106), ('мил', 2.791670074398225), ('лучш', 2.7767779187864705), ('вообщ', -1.9195604110562385), ('сук', -2.311246049541277), ('нах', -2.4755237119964275), ('пиздец', -2.7423784209095072), ('блят', -3.3267499784489982)]\n"
     ]
    }
   ],
   "source": [
    "print('Логистическая регрессия:')\n",
    "print(classification_report(y_test, y_predicted_lr_stemmed))\n",
    "print('ROC-AUC:', roc_auc_score(y_test, y_predicted_lr_stemmed))\n",
    "print()\n",
    "\n",
    "coefs = lr_stemmed.coef_[0]\n",
    "indexes_lr = coefs.argsort()\n",
    "\n",
    "top_bad_words_lr = []\n",
    "for i in indexes_lr[:5]:\n",
    "    top_bad_words_lr.append((words[i], coefs[i]))\n",
    "    \n",
    "\n",
    "top_good_words_lr = []\n",
    "for i in indexes_lr[-5:]:\n",
    "    top_bad_words_lr.append((words[i], coefs[i]))\n",
    "\n",
    "print(\"Пять слов с набольшим позитивным окрасом:\")\n",
    "print(top_good_words_lr)\n",
    "print(\"Пять слов с набольшим негативным окрасом:\")\n",
    "print(top_bad_words_lr[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Случайный лес:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.71      0.74       660\n",
      "           1       0.65      0.72      0.68       483\n",
      "\n",
      "    accuracy                           0.71      1143\n",
      "   macro avg       0.71      0.72      0.71      1143\n",
      "weighted avg       0.72      0.71      0.72      1143\n",
      "\n",
      "ROC-AUC: 0.715829098437794\n",
      "\n",
      "Десять слов с набольшим окрасом:\n",
      "['пиздец', 'классн', 'мил', 'крут', 'эт', 'прекрасн', 'лучш', 'блят', 'красив', 'любл']\n"
     ]
    }
   ],
   "source": [
    "print('Случайный лес:')\n",
    "print(classification_report(y_test, y_predicted_forest_stemmed))\n",
    "print('ROC-AUC:', roc_auc_score(y_test, y_predicted_forest_stemmed))\n",
    "print()\n",
    "\n",
    "coefs = forest_stemmed.feature_importances_\n",
    "indexes_lr = coefs.argsort()\n",
    "\n",
    "top_words_forest = []\n",
    "for i in indexes_lr[-10:]:\n",
    "    top_words_forest.append(words[i])\n",
    "\n",
    "print(\"Десять слов с набольшим окрасом:\")\n",
    "print(top_words_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem = Mystem()\n",
    "def lemmatize(text):\n",
    "    lemmas = mystem.lemmatize(text)\n",
    "    stop_words = stopwords.words(\"russian\")\n",
    "    lemmas = [lemma for lemma in lemmas if lemma not in stop_words and lemma.isalnum() or emoji.is_emoji(lemma) or lemma == '!' or lemma == '?']\n",
    "    lemmatized_text = ' '.join(lemmas)\n",
    "\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_train = train_pd_f.copy()\n",
    "lemmatized_train['text'] = lemmatized_train['text'].apply(lemmatize)\n",
    "\n",
    "lemmatized_test = test_pd_f.copy()\n",
    "lemmatized_test['text'] = lemmatized_test['text'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "x_train_lemmatized = vectorizer.fit_transform(lemmatized_train['text'])\n",
    "x_test_lemmatized = vectorizer.transform(lemmatized_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "x_train_lemmatized_tfidf = tfidf_transformer.fit_transform(x_train_lemmatized)\n",
    "x_test_lemmatized_tfidf = tfidf_transformer.transform(x_test_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_lemmatized = LogisticRegression()\n",
    "lr_lemmatized.fit(x_train_lemmatized_tfidf, y_train)\n",
    "y_predicted_lr_lemmatized = lr_lemmatized.predict(x_test_lemmatized_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_lemmatized = RandomForestClassifier()\n",
    "forest_lemmatized.fit(x_train_lemmatized_tfidf, y_train)\n",
    "y_predicted_forest_lemmatize = forest_lemmatized.predict(x_test_lemmatized_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.92      0.82       660\n",
      "           1       0.84      0.55      0.66       483\n",
      "\n",
      "    accuracy                           0.76      1143\n",
      "   macro avg       0.79      0.74      0.74      1143\n",
      "weighted avg       0.78      0.76      0.75      1143\n",
      "\n",
      "ROC-AUC: 0.7356907585168455\n",
      "\n",
      "Пять слов с набольшим позитивным окрасом:\n",
      "['милый', 'прекрасный', 'красивый', 'хороший', 'любить']\n",
      "Пять слов с набольшим негативным окрасом:\n",
      "['сука', 'умирать', 'нахуй', 'пиздец', 'блять']\n"
     ]
    }
   ],
   "source": [
    "print('Логистическая регрессия:')\n",
    "print(classification_report(y_test, y_predicted_lr_lemmatized))\n",
    "print('ROC-AUC:', roc_auc_score(y_test, y_predicted_lr_lemmatized))\n",
    "print()\n",
    "\n",
    "coefs = lr_lemmatized.coef_[0]\n",
    "indexes_lr = coefs.argsort()\n",
    "\n",
    "top_bad_words_lr = []\n",
    "for i in indexes_lr[:5]:\n",
    "    top_bad_words_lr.append(words[i])\n",
    "\n",
    "top_good_words_lr = []\n",
    "for i in indexes_lr[-5:]:\n",
    "    top_good_words_lr.append(words[i])\n",
    "\n",
    "print(\"Пять слов с набольшим позитивным окрасом:\")\n",
    "print(top_good_words_lr)\n",
    "print(\"Пять слов с набольшим негативным окрасом:\")\n",
    "print(top_bad_words_lr[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Случайный лес:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.71      0.75       660\n",
      "           1       0.65      0.75      0.69       483\n",
      "\n",
      "    accuracy                           0.72      1143\n",
      "   macro avg       0.72      0.73      0.72      1143\n",
      "weighted avg       0.73      0.72      0.72      1143\n",
      "\n",
      "ROC-AUC: 0.7257011104837191\n",
      "\n",
      "Десять слов с набольшим окрасом:\n",
      "['прекрасный', 'вау', 'любовь', 'нравиться', 'пиздец', 'это', 'красивый', 'блять', 'хороший', 'любить']\n"
     ]
    }
   ],
   "source": [
    "print('Случайный лес:')\n",
    "print(classification_report(y_test, y_predicted_forest_lemmatize))\n",
    "print('ROC-AUC:', roc_auc_score(y_test, y_predicted_forest_lemmatize))\n",
    "print()\n",
    "\n",
    "coefs = forest_lemmatized.feature_importances_\n",
    "indexes_lr = coefs.argsort()\n",
    "\n",
    "top_words_forest = []\n",
    "for i in indexes_lr[-10:]:\n",
    "    top_words_forest.append(words[i])\n",
    "\n",
    "print(\"Десять слов с набольшим окрасом:\")\n",
    "print(top_words_forest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
